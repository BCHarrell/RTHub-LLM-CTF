# NOTE:
# Pydantic supports other OpenAI compatible providers, but they require defining
# the provider when creating the agent if the LLM is HOSTED REMOTELY. I didn't 
# add a switch statement for that (yet?) so if you want to use DeepSeek /
# OpenRouter / etc., modify the basic_agent (or write your own) to create the
# right model object. See the docs/ folder.
#
# LOCALLY HOSTED LLMs that are OpenAI compatible are already supported, presuming 
# they don't require an API key. If they do, modify the basic_agent to add that.
# Add the URL in the config.yaml where appropriate. This mostly applies to Ollama.
#
# Pydantic docs for providers: https://ai.pydantic.dev/models/openai
# Just copy that code and replace `agent = Agent(...)` with the model you created.

# Models supported by Pydantic
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
GEMINI_API_KEY=
GROQ_API_KEY=
MISTRAL_API_KEY=
CO_API_KEY= # Cohere is bugged in Pydantic... I submitted a PR to fix it. 
            # See docs/usage.md for workaround if you really want to use it. 

# Bedrock Requirements - UNTESTED but /should/ work based on Pydantic docs
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_DEFAULT_REGION=
